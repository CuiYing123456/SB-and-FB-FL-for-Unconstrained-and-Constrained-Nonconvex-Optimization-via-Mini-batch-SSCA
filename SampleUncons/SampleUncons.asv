%% Initialization
clear ; close all; clc

%% =========== Part 1: Loading and Visualizing Data =============
fprintf('Loading Data ...\n')

% load training data set
fid = fopen('.\MNIST_data\train-images.idx3-ubyte', 'rb');
A_train_img = fread(fid, 4, 'uint32', 'ieee-be');
img_train_num = A_train_img(2);
row_num = A_train_img(3);
col_num = A_train_img(4);
img_train = fread(fid, [row_num*col_num, img_train_num], 'unsigned char', 'ieee-be');
img_train = (img_train/255);

% load training label set
fid = fopen('.\MNIST_data\train-labels.idx1-ubyte', 'rb');
A_train_label = fread(fid, 2, 'uint32', 'ieee-be');
lab_train_num = A_train_label(2);
lab_train = fread(fid, lab_train_num, 'unsigned char',  'ieee-be');

% load test data set
fid = fopen('.\MNIST_data\t10k-images.idx3-ubyte', 'rb');
A_test_img = fread(fid, 4, 'uint32', 'ieee-be');
img_test_num = A_test_img(2);
img_test = fread(fid, [row_num*col_num, img_test_num], 'unsigned char',  'ieee-be');
img_test = (img_test/255);

% load test label set
fid = fopen('.\MNIST_data\t10k-labels.idx1-ubyte', 'rb');
A_test_label = fread(fid, 2, 'uint32', 'ieee-be');
lab_test_num = A_test_label(2);
lab_test = fread(fid, lab_test_num, 'unsigned char',  'ieee-be');

%% Setup the parameters
% network hyperparameter
input_layer_size  = 784;  % 28 x 28 Input Images of Digits
hidden_layer_size = 128;   % number of hidden units
num_labels = 10;          % 10 labels, from 1 to 10

% initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size);
% initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels);
% J_epoch_init = nnCostFunction(initial_Theta1, initial_Theta2, num_labels, img_train, lab_train, lambda);
%
% pred = Predict(initial_Theta1, initial_Theta2, img_train);
% pred = pred';
% Accuracy_epoch_init = mean(double(pred == lab_train)) * 100;
% save(['initial_Theta1.mat'], 'initial_Theta1');
% save(['initial_Theta2.mat'], 'initial_Theta2');
% save(['J_epoch_init.mat'], 'J_epoch_init');
% save(['Accuracy_epoch_init.mat'], 'Accuracy_epoch_init');

% load initial point
load ('.\InitialData\initial_Theta1.mat');
load ('.\InitialData\initial_Theta2.mat');
load ('.\InitialData\J_epoch_init.mat');
load ('.\InitialData\Accuracy_epoch_init.mat');

% simulation parameters
lambda = 0;     % regularization
N = 10;     % N workers
L=1;
tau=0.1;

% % local dataset
% local_img = zeros(row_num * col_num, img_train_num/N, N);
% local_lab = zeros( img_train_num/N, N);
% for i = 1:N
%     mini_idx = [ img_train_num/N * (i - 1) + 1:  img_train_num/N * i];
%     local_img(:, :, i) = img_train(:, mini_idx);
%     local_lab(:, i) = lab_train(mini_idx);
% end
T=1000;
B_list = [1, 10, 100];
a1_list = [0.4, 0.6, 0.9];
a2_list = [0.4, 0.9, 0.9];
alpha_list = [0.4, 0.3, 0.3];
for i = 1 : 3
    B = B_list(i);  a1 = a1_list(i);  a2 = a2_list(i);  alpha = alpha_list(i);
    for iter=1:L
        fprintf('\nTraining Neural Network... \n')
        Theta1a=initial_Theta1;
        Theta2a=initial_Theta2;
        Theta1b=initial_Theta1;
        Theta2b=initial_Theta2;
        Theta1c=initial_Theta1;
        Theta2c=initial_Theta2;
        Total_grad1=zeros(size(Theta1a));
        Total_grad2=zeros(size(Theta2a));
        grad1 = zeros(size(Theta1c));
        grad2 = zeros(size(Theta2c));
        % training
        for t=1:T
            rho = a1/t^alpha;  gamma = a2/t^alpha+0.05;  lr = 1/t^0.15;  mom = 0.1; eta = 1;
            % sampling
            rand_set = randperm(img_train_num); % 训练集乱序
            minibatch = rand_set(1:B*N);        % 取一个minibatch
            X = img_train(:, minibatch);      % minibatch sample set
            y = lab_train(minibatch);     % minibatch label set
            
            % SSCA update
            [Theta1_local, Theta2_local] = nnGradient(Theta1a,Theta2a, input_layer_size, hidden_layer_size, num_labels, X, y, lambda);  %梯度
            Total_grad1 = (1-rho)*Total_grad1+rho*(Theta1_local-2*tau*Theta1a);
            Total_grad2 = (1-rho)*Total_grad2+rho*(Theta2_local-2*tau*Theta2a);
            J = nnCostFunction(Theta1a, Theta2a, num_labels, img_train, lab_train, lambda);        %cost function
            J_step1(t,iter)=J;    %过程中cost function记录
            Theta1 = -0.5*Total_grad1/tau;
            Theta2 = -0.5*Total_grad2/tau;
            Theta1a = (1-gamma)*Theta1a+gamma*Theta1;     %model update
            Theta2a = (1-gamma)*Theta2a+gamma*Theta2;     %model update
            pred = Predict(Theta1a, Theta2a, img_test);
            pred = pred';
            accur1(t,iter)=mean(double(pred == lab_test)) * 100;
            fprintf('\nTest accuracy of step %d (SSCA): %f\n', t, mean(double(pred == lab_test)) * 100);    %准确率
           
            % SGD update
            [Theta1_local, Theta2_local] = nnGradient(Theta1b,Theta2b, input_layer_size, hidden_layer_size, num_labels, X, y, lambda);  %梯度
            J = nnCostFunction(Theta1b, Theta2b, num_labels, img_train, lab_train, lambda);        %cost function
            J_step2(t,iter)=J;    %过程中cost function记录
            Theta1b = Theta1b-lr*Theta1_local;   %model update
            Theta2b = Theta2b-lr*Theta2_local;
            pred = Predict(Theta1b, Theta2b, img_test);
            pred = pred';
            accur2(t,iter)=mean(double(pred == lab_test)) * 100;
            fprintf('Test accuracy of step %d (SGD): %f\n', t, mean(double(pred == lab_test)) * 100);    %准确率 
        
            % SGD-momentum update
            [Theta1_local, Theta2_local] = nnGradient(Theta1c,Theta2c, input_layer_size, hidden_layer_size, num_labels, X, y, lambda);  %梯度
            J = nnCostFunction(Theta1c, Theta2c, num_labels, img_train, lab_train, lambda);        %cost function
            J_step3(t,iter)=J;    %过程中cost function记录
            grad1 = mom*grad1 + (1-mom)*Theta1_local;
            grad2 = mom*grad2 + (1-mom)*Theta2_local;
            Theta1c = Theta1c-0.9*grad1;   %model update
            Theta2c = Theta2c-0.9*grad2;
            pred = Predict(Theta1c, Theta2c, img_test);
            pred = pred';
            accur3(t,iter)=mean(double(pred == lab_test)) * 100;
            fprintf('Test accuracy of step %d (SGD-mom): %f\n', t, mean(double(pred == lab_test)) * 100);    %准确率 
        end
    end
    fprintf('\nComplete training of iteration %d for B=%d! \n', iter, B);
    
    a_mean1=mean(accur1,2);
    cost_mean1=mean(J_step1,2);
    save(strcat('.\SaveData\SUa1_B', num2str(B), '.mat'), 'a_mean1');
    save(strcat('.\SaveData\SUc1_B', num2str(B), '.mat'), 'cost_mean1');
    
    a_mean2=mean(accur2,2);
    cost_mean2=mean(J_step2,2);
    save(strcat('.\SaveData\SUa2_B', num2str(B), '.mat'), 'a_mean2');
    save(strcat('.\SaveData\SUc2_B', num2str(B), '.mat'), 'cost_mean2');
    
    a_mean3=mean(accur3,2);
    cost_mean3=mean(J_step3,2);
    save(strcat('.\SaveData\SUa3_B', num2str(B), '.mat'), 'a_mean3');
    save(strcat('.\SaveData\SUc3_B', num2str(B), '.mat'), 'cost_mean3');
    %     save(sprintf('%s%d','.\SaveData\SampleUncons_B',B))
end

%% ====== FedAvg ========
T = 1000;
% 
% B = 100;
% for iter=1:L
%     fprintf('\nTraining Neural Network... \n')
%     Theta1c=initial_Theta1;
%     Theta2c=initial_Theta2;
%     Total_grad1=zeros(size(Theta1c));
%     Total_grad2=zeros(size(Theta2c));
%     % training
%     for t=1:T
%         rho = a1/t^alpha;  gamma = a2/t^alpha+0.05;
% %         % sampling
% %         rand_set = randperm(img_train_num); % 训练集乱序
% %         minibatch = rand_set(1:B*N);        % 取一个minibatch
%         X = img_train(:, :);      % minibatch sample set
%         y = lab_train(:);     % minibatch label set
%         
%         % SSCA update
%         [Theta1_local, Theta2_local] = nnGradient(Theta1c,Theta2c, input_layer_size, hidden_layer_size, num_labels, X, y, lambda);  %梯度
%         Total_grad1 = (1-rho)*Total_grad1+rho*(Theta1_local-2*tau*Theta1c);
%         Total_grad2 = (1-rho)*Total_grad2+rho*(Theta2_local-2*tau*Theta2c);
%         J = nnCostFunction(Theta1c, Theta2c, num_labels, img_train, lab_train, lambda);        %cost function
%         J_step3(t,iter)=J;    %过程中cost function记录
%         Theta1 = -0.5*Total_grad1/tau;
%         Theta2 = -0.5*Total_grad2/tau;
%         Theta1c = (1-gamma)*Theta1c+gamma*Theta1;     %model update
%         Theta2c = (1-gamma)*Theta2c+gamma*Theta2;     %model update
%         pred = Predict(Theta1c, Theta2c, img_test);
%         pred = pred';
%         accur3(t,iter)=mean(double(pred == lab_test)) * 100;
%         fprintf('\nTest accuracy of step %d (SSCA): %f\n', t, mean(double(pred == lab_test)) * 100);    %准确率
%     end
% end
% fprintf('\nComplete training of iteration %d for B=%d! \n', iter, B);
% a_mean3=mean(accur3,2);
% a_mean3=mean(accur3,2);
% cost_mean3=mean(J_step3,2);
% cost_mean3=mean(J_step3,2);
% save(strcat('.\SaveData\SUa3_B', num2str(B), '.mat'), 'a_mean3');
% save(strcat('.\SaveData\SUc3_B', num2str(B), '.mat'), 'cost_mean3');


% FedAvg
B_FedAvg = [50, 20];
E_FedAvg = [2, 5];
% local dataset
local_img = zeros(row_num * col_num, img_train_num/N, N);
local_lab = zeros( img_train_num/N, N);
for i = 1:N
    mini_idx = [ img_train_num/N * (i - 1) + 1:  img_train_num/N * i];
    local_img(:, :, i) = img_train(:, mini_idx);
    local_lab(:, i) = lab_train(mini_idx);
end

for i = 1 : 2
    B = B_FedAvg(i);  E = E_FedAvg(i);
    for iter=1:L
        fprintf('\nTraining Neural Network... \n')
        Theta1d=initial_Theta1;
        Theta2d=initial_Theta2;
        Theta1_local = zeros(size(Theta1d, 1), size(Theta1d, 2), N);
        Theta2_local = zeros(size(Theta2d, 1), size(Theta2d, 2), N);
        % training
        for t=1:T
            lr = 1/t^0.15;
%             Theta1_grad = 0;  Theta2_grad = 0;
            for n = 1 : N
            Theta1_local(:, :, n) = Theta1d;
            Theta2_local(:, :, n) = Theta2d;
            end
             % FedAvg update
            J = nnCostFunction(Theta1d, Theta2d, num_labels, img_train, lab_train, lambda);        %cost function
            J_step4(t,iter)=J;    %过程中cost function记录
            for e = 1 : E  % local training
                % sampling
                rand_idx = randperm(img_train_num/N); % 训练集乱序
                rand_idx = rand_idx(1:B);
                for n = 1 : N
                    X = local_img(:, rand_idx, n);
                    y = local_lab(rand_idx, n);
                    [local_Grad1, local_Grad2] = nnGradient(Theta1_local(:,:,n),Theta2_local(:,:,n), input_layer_size, hidden_layer_size, num_labels, X, y, lambda);  %梯度
                    Theta1_local(:, :, n) = Theta1_local(:, :, n) - lr * local_Grad1;
                    Theta2_local(:, :, n) = Theta2_local(:, :, n) - lr * local_Grad2;
                end
            end
            Theta1d = mean(Theta1_local, 3);   %model update
            Theta2d = mean(Theta2_local, 3);
            pred = Predict(Theta1d, Theta2d, img_test);
            pred = pred';
            accur4(t,iter)=mean(double(pred == lab_test)) * 100;
            fprintf('Test accuracy of step %d (FedAvg): %f\n', t, mean(double(pred == lab_test)) * 100);    %准确率
        end
    end
    fprintf('\nComplete training of iteration %d for B=%d! \n', iter, B);
    
    a_mean4=mean(accur4,2);
    cost_mean4=mean(J_step4,2);
    save(strcat('.\SaveData\SUa4_B', num2str(B), '.mat'), 'a_mean4');
    save(strcat('.\SaveData\SUc4_B', num2str(B), '.mat'), 'cost_mean4');
end
